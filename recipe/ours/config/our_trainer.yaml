hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  gen_batch_size: ${data.train_batch_size}


reward_model:
  reward_manager: deepscaler
  overlong_buffer: 
    enable: False # We try to avoid forgetting to set enable
    len: 0
    penalty_factor: 0.0
    log: False

algorithm:
  filter_groups:
    enable: False # We try to avoid forgetting to set enable
    metric: null # acc / score / seq_reward / seq_final_reward / ...
    max_num_gen_batches: 0 # Non-positive values mean no upper limit

trainer:
  project_name: verl-our


tasksampler:
  framework: 0 #0: uniform; 1: gdro; 2: cvar; 3: mpts
  ts_ratio: 1.0
  gamma_0: 1.0
  gamma_1: 3.0
  gamma_2: 0.0
  mpts_proxy_lr: 0.005
  mpts_kl_weight: 0.0001
  warmup: 0
  no_add_random: False
  rejection_sampling_ratio: 0.0  #0.0 means no rejection sampling; 1.0 means only top B samples are kept
  save_data: False
  bandit_sample_strategy: "threshold" #threshold/topk/softmax
  bandit_target_mean: 0.5
  bandit_target_std: 0.0
  bandit_lower_bound: 0.3
  bandit_upper_bound: 0.7
  bandit_upper_decay_steps: 0 #700
  bandit_upper_decay_lower: 0.55
  bandit_sample_std: 0.05
  bandit_init: False
  bandit_init_dir: "/home/quyun/deepscaler/outputs/init_eval_train_1/index_score.json"
  bandit_load_dir: ""

